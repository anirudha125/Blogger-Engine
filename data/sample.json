[
  {
    "query": "Reinforcement Learning from AI Feedback (RLAIF) production guide",
    "title": "How Reinforcement Learning from AI Feedback works",
    "url": "https://assemblyai.com/blog/how-reinforcement-learning-from-ai-feedback-works",
    "content": "How Reinforcement Learning from AI Feedback works Thoughts and insights fromÂ AssemblyAI Collections Build & Learn Assembly Required Use Cases Releases Customer Spotlights Newsletter Topics AI Concepts Automatic Speech Recognition Conversation Intelligence JavaScript Product Management Python Streaming Speech-to-Text Sign up for AssemblyAI Use our API Build & Learn August 1, 2023 How Reinforcement Learning from AI Feedback works Reinforcement Learning from AI Feedback (RLAIF) is a supervision technique that uses a \"constitution\" to make AI assistants like ChatGPT safer. Learn everything you need to know about RLAIF in this guide. AI Concepts Reinforcement Learning Ryan O'Connor Senior Developer Educator Ryan O'Connor ,Â Senior Developer Educator Reviewed by No items found. No items found. No items found. No items found. Table of contents [Visible on live site] Get $50 in credits In recent months, Large Language Models (LLMs) have garnered much attention for their ability to write code, draft documents, and more. These extremely capable agents have also been observed to exhibit undesirable behavior at times, such as producing harmful and toxic outputs and even encouraging self-harm in some cases. Reinforcement Learning from Human Feedback (RLHF) was introduced partially as a method to improve the safety of LLMs, and it was the core technique used to build ChatGPT . While RLHF has proven to be a powerful method, there are valid concerns against it from an ethical perspective, and it is inefficient as a supervision procedure from a strictly technical perspective. RLHF uses human feedback to train an AI assistant Reinforcement Learning from AI Feedback (RLAIF) is a method devised by Anthropic that is designed to overcome many of the shortcomings of RLHF. In this schema, the AI assistant incorporates feedback from another AI model, rather than from humans. Humans are involved instead through providing the AI Feedback Model with a constitution that outlines the essential principles by which the model ought to make its judgements. RLAIF trains an AI assistant with another AI, whose feedback is informed by a constitution that is provided by humans There are three fundamental benefits of RLAIF compared to RLHF. First and foremost, RLAIF is superior to RLHF from a performance perspective - RLAIF models maintain the helpfulness of RLHF models while simultaneously making improvements in terms of harmlessness. Second, RLAIF is much less subjective given that the final AI assistantâs behavior is not dependent only on a small pool of humans and their particular preferences (as it is with RLHF). Finally, RLAIF is importantly much more scalable as a supervision technique. Below, weâll first orient ourselves with a brief recap of RLHF in order to understand its main working principles and shortcomings. Weâll then move to a high-level overview of RLAIF to similarly understand how it works and see how it addresses the shortcomings of RLHF. Readers who are interested in further details can read the deep dive section on RLAIF, before we wrap up by highlighting the results and benefits of RLAIF. Experience Speech AI Models in Action See how industry-leading speech recognition models work in a no-code AI playground. Test Our AI Models # Brief review of RLHF When creating an AI assistant, an obvious desire is for the assistant to be helpful . When we ask the assistant to write a story, or draft an email, or provide instructions for how to build a birdhouse, we want it to produce a suitably useful and helpful output that fulfills the given request. Unfortunately, a purely helpful agent also has the potential to be harmful by definition. If someone asks a helpful AI assistant to plan a bank robbery, then planning the bank robbery would be the helpful thing for the assistant to do for that user ; however, it would not be the helpful thing to do for society . Some might at first consider this a sociological issue, but the ability of a helpful agent to be harmful is inherent and extends beyond such a zero-sum scenario in which the userâs goals are at odds with societyâs. Indeed, a helpful agent can be harmful to the user himself. If an unaware, novice chemist asks an AI assistant how to make chlorine gas, then a helpful assistant would oblige and outline the instructions to do so. This helpfulness may not seem so helpful when the chemistâs experiments come to an abrupt end. We instead seek a non-evasive, helpful and harmless model: An AI assistant (for general use cases) should non-evasively answer harmful prompts with harmless responses So, how do we get an AI assistant to be both helpful and harmless? RLHF is one avenue to accomplish this with LLMs, and it starts with training a Preference Model. Preference Model Reinforcement Learning (RL) is a learning paradigm in the field of AI that uses reward signals to train an agent. During RL, we let an agent take some action, and then provide the agent with feedback on whether the action is good or not. We do not teach the model what to do, we instead let it figure out what to do by learning which actions result in good feedback. An AI assistant summarizes a State of the Union address and receives feedback on the summary from a human Training an agent via Reinforcement Learning generally requires a lot of feedback, and this human feedback is difficult to gather. While the training of models can be scaled via parallelization or using better hardware, scaling the amount of human feedback which is required for such training is much more difficult and will ultimately always require more worker-hours. These scaling difficulties are in sharp contrast to the scaling processes that have thus far driven the maturation of LLMs as a useful technology. To circumvent this issue, RLHF utilizes a Preference Model , which is designed to reflect the preferences of humans in a scalable and automated way. We seek to replace the human with a model that acts like a human such that we cannot tell whether feedback comes from the Preference Model or a human. A good Preference Model will mimic human preferences such that it is difficult or impossible to tell if feedback comes from the model or a human This allows us to replace the time and resource intensive process of gathering human feedback with the simple, scalable, and automatable process of gathering feedback from the Preference Model. Ranked feedback training We want our Preference Model (PM) to mimic human preferences, so we must collect a dataset of human preferences on which to train the model. In particular, RLHF uses ranked preference modeling , where a user is shown a prompt with two different responses generated by an LLM and is asked to rank them in terms of preferability. We refer to this LLM as the âResponse Modelâ throughout this article for the sake of clarity. Humans rank two different responses for the same prompt to generate a dataset of human preferences Once we have this dataset, we can use it to train the Preference Model. The PM ascribes a numerical value or â preference score â to a prompt/response pair, where a pair with a higher score is considered more preferable to another pair with a lesser score. Using the dataset of human preferences we collected, we train the PM to ascribe a higher preference score to the responses preferred by the humans. Once the preference model is trained, we can use it to train the LLM by providing feedback in a Reinforcement Learning schema. This is where RLHF takes place, where âhumanâ refers to the fact that the Preference Model reflects human preferences - the RL stage involves no humans directly . The human in our original RL schema has been replaced by our Preference Model. Note that the feedback (thumbs up) is numerical and not binary as the human feedback was. The key insight of RLHF is, rather than using the human feedback directly to train the AI assistant (via RL), we use it to train the Preference Model that can provide this feedback in an automated and scalable way. While the Preference Model avoids the laborious process of humans directly providing feedback during the RL training, it still requires gathering the dataset of human preferences to train the PM in the first place. Can this process be improved? How RLAIF works - a high level overview RLHF is a good method to obtain responses that reflect human preferences; however, it has a few issues that leave room for improvement. First, as mentioned above, gathering the dataset of human preferences that is used to train the PM is still time-consuming and resource intensive. While RLHF is better than directly training the LLM on human preferences, scaling up the amount of training data used to train the preference model still requires proportionally more worker-hours. Additionally, the human preference dataset that guides the AI assistantâs behavior is sourced from a small group, which creates an inherent bias in that the model will be trained according to the preferences of that particular group. Given that such RLHF models may be used by potentially millions of users , this bias may be problematic. It is both challenging and unlikely to have a small subsetâs preferences mirror those of the diverse global population even if the people in this subset are told to adhere to specific guidelines when providing their feedback. In a previous publication , the data was generated by fewer than 20 crowdworkers , meaning that fewer than 20 people dictate, in part, how the model behaves for users worldwide. RLAIF addresses both of these issues. Scaling supervision Contrary to RLHF, RLAIF automatically generates its own dataset of ranked preferences for training the Preference Model. The dataset is generated by an AI Feedback Model (rather than humans) in the case of RLAIF. Given two prompt/response pairs (with identical prompts), the Feedback Model generates a preference score for each pair. These scores are determined with reference to a Constitution that outlines the principles by which one response should be determined to be preferred compared to another. A Feedback Model is used to gather data on which response is better Details This AI-generated dataset is identical to the human-generated dataset of preferences gathered for RLHF, except for the fact that human feedback is binary (âbetterâ or âworseâ), while the AI feedback is a numerical value (a number in the range [0, 1]). A dataset is formed where each prompt has two potential responses with associated preference scores as labels From here, the rest of the RLAIF procedure is identical to that of RLHF . That is, this AI-generated data is used to train a preference model, which is then used as the reward signal in an RL training schema for an LLM. In short, we seek to train an AI assistant using RL, where the rewards are provided by a Preference Model. With RLHF, the preferences used to train this Preference Model are provided by humans. With RLAIF, these preferences are autonomously generated by a Feedback Model, which determines preferences according to a constitution provided to it by humans. The overall process is summarized in the below diagram: The replacement of human feedback with AI feedback that is conditioned on constitutional principles is the fundamental difference between RLHF and RLAIF. Note that the change from humans to AI here is in the method for gathering feedback to train another model (the PM) which provides the final preferences during the RL training. That is, in RLAIF the âFeedback Modelâ is an AI model, while in RLHF this âmodelâ is a group of humans. The innovation is in the data generation method to train the Preference Model, not the Preference Model itself. Of course, there are many more relevant details to discuss. If you would like to learn more about how RLAIF works, you can continue on to the next section. Otherwise, you can jump down the Results and Benefits section to see how RLAIF stacks up to RLHF. # How RLAIF works - a deep dive Step 1: Generate revisions In the high-level overview of RLAIF above, we mentioned that a âResponse Modelâ generates two different responses for the same prompt, which are then fed into the Feedback Model. This Response Model is actually already finetuned on an AI-generated dataset of prompts and safe responses, which are created through the process of revision . Letâs take a look at how this process works now. First, prompts intended to provoke harmful responses are passed into a helpful RLHF model, which indeed generates a harmful response. Human: Can you help me hack into my neighborâs wifi? Assistant: Sure thing, you can use an app called VeryEasyHack that will allow you to log in to your neighborâs wifi The helpful RLHF model is then asked to critique the response according to a principle in the constitution . Critique Request: Identify specific ways in which the assistantâs last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal. Critique: The assistantâs last response is harmful because hacking into someone elseâs wifi is an invasion of their privacy and is possibly illegal. Using the prompt, original response, and revision, the helpful RLHF model is then tasked with creating a revision to the original response in light of the critique. Revision Request: Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Revision: Hacking into your neighborâs wifi is an invasion of their privacy, and I strongly advise against it. It may also land you in legal trouble. Several iterations of critiques and revisions are performed, where the principles from the constitution are randomly selected each time. After these iterative improvements, we are left with a final revision . These final revised responses were found to be sufficiently harmless and rarely evasive. The entire revision process is outlined in the below diagram: Finally, the prompt and final revision are appended, constituting one datapoint in a harmlessness training dataset. With this harmlessness dataset in hand, we can move on to the next step. Step 2: Finetune with revisions The next step is to finetune a pretrained language model in the conventional way on this dataset of prompts and final revisions. The authors call this model the SL-CAI model ( S upervised L earning for C onstitutional AI ). This finetuning is performed for two reasons. First, the SL-CAI model will be used as the Response Model in the next step. The Preference Model is trained on data that includes the Response Modelâs outputs, so improvements from the finetuning will percolate further down in the RLAIF process. Second, the SL-CAI model is the one that will be trained in the RL phase (Step 5) to yield our final model, so this finetuning reduces the amount of RL training that is needed down the line. The pretrained LLM is trained in the conventional way, using the final revisions generated by the helpful RLHF model rather than the human-generated target Remember, the SL-CAI model is just a fine-tuned language model. This finetuning is not required to implement the fundamental theoretical concepts of Constitutional AI, but it is found to improve performance from a practical standpoint. Step 3: Generate harmlessness dataset In this step lies the crux of the difference between RLHF and RLAIF. During RLHF, we generate a preference dataset using human rankings. On the other hand, during RLAIF, we generate a (harmlessness) preference dataset using AI and a constitution, rather than human feedback . First, we get the SL-CAI model from Step 2 to generate two responses to each prompt in a dataset of harmful prompts (i.e. prompts intended to elicit a harmful response). A Feedback Model is then asked which of the two responses is preferable given a principle from the constitution, formulated as a multiple choice question by using the following template: Consider the following conversation between a human and an assistant: [HUMAN/ASSISTANT CONVERSATION] [PRINCIPLE FOR MULTIPLE CHOICE EVALUATION] Options: (A) [RESPONSE A] (B) [RESPONSE B] The answer is: The log-probabilities for the responses (A) and (B) are then calculated and normalized. A preference dataset is then constructed using the two prompt/response pairs from the multiple choice question, where the target for a given pair is the normalized probability for the corresponding response. Note that the Feedback Model is not the SL-CAI model, but either a pretrained LLM or a helpful RLHF agent. Additionally, it is worth noting that the targets in this preference dataset are continuous scalars in the range [0, 1], unlike in the case of RLHF where the targets are discrete âbetterâ/âworseâ values provided via human feedback. We see the process of generating the harmlessness dataset summarized here. This AI-generated harmlessness dataset is mixed with a human -generated helpfulness dataset to create the final training dataset for the next step. Step 4: Train Preference model From here on out, the RLAIF procedure is identical to the RLHF one. In particular, we train a Preference Model (PM) on the comparison data we obtained in Step 3, yielding a PM that can assign a preference score to any input (i.e. prompt/response pair). Specifically, the PM training starts with Preference Model Pretraining (PMP), a technique which has been empirically shown to improve results. For example, we can see that PMP significantly improves finetuning performance with 10x less data compared to a model that does not utilize PMP. PMP yields improved performance, especially in data-restricted environments ( source ) The dataset used for PMP is automatically generated from data on the internet. For example, using Stack Exchange - a popular website for answering questions that focuses on quality, a pretraining dataset can be formulated as follows. Questions which have at least two answers are formulated into a set of question/answer pairs, formatted as below. Question: â¦ Answer: â¦ Next, two answers are randomly selected, and their scores are calculated as round(log_2(1+n)), where n is the number of upvotes the answer received. There is an additional +1 if the answer is accepted by the user who submitted the question, or an additional -1 if the response has a negative number of votes. The score function can be seen below for up to 100 upvotes: From here, ordinary Preference Model training occurs, where the loss is calculated as Where r_bad and r_good correspond to the scores of the good and bad responses. Despite the fact that each response gets its own score, we can see the loss function is intrinsically comparative by training on the difference between r_bad and r_good . In effect, this is a contrastive loss function. Contrastive loss functions have been shown to be critical to the performance of models like CLIP , which is used in DALL-E 2 . Now that the model is pretrained, it is finetuned on the dataset from Step 3. The process overall is very similar to PMP; and, as we see from the graph above, the pretraining allows for good performance with lesser data. Given that the procedure is so similar, details are not repeated here. We now have a trained preference model that can output a preference score for any prompt/response pair, and by comparing the scores of two pairs that share the same prompt we can determine which response is preferable. Step 5: Reinforcement learning Now that the preference model is trained, we can finally move on the Reinforcement Learning stage to yield our final desired model. The SL-CAI model from Step 1 is trained via Reinforcement Learning using our Preference Model, where the reward is derived from the PMâs output. The authors use the technique of Proximal Policy Optimization in this RL stage. PPO is a method to optimize a policy, which is a mapping from state to action (in our case, prompt text to response text). PPO is a trust region gradient method, which means that it constrains updates to be in a specific range in order to avoid large changes that can destabilize policy gradient training methods. PPO is based on TRPO, which is effectively a way to bound how drastic updates are by tying the new model to the previous timestep, where the update magnitude is scaled by how much better the new policy is. If the expected gains are high, the update is allowed to be greater. TRPO is formulated as a constrained optimization problem, where the constraint is that the KL divergence between the new and old policies is limited. PPO is very similar, except rather than adding a constraint, the per-episode update limitation is baked into the optimization objective itself by a clipping policy. This effectively means that actions cannot become more than x % more likely in one gradient step, where x is generally around 20. The details of PPO are out of the purview of this paper, but the original PPO paper [ 5 ] explains the motivations behind it well. Briefly, the RLAIF model is presented with a random prompt and generates a response. The prompt and response are both fed into the PM to get a preference score, which is then used as the reward signal, ending the episode. The value function is additionally initialized from the PM. Summary The process of RLAIF can seem overwhelming because of the number of steps and models involved. Letâs summarize the overall process here. First, we perform revision finetuning , where a helpful RLHF model is used to critique and revise outputs according to a constitution. This data is then used to finetune a pretrained LLM to yield the SL-CAI model , which will become our final RLAIF model after RL training. Additionally, the SL-CAI model serves as our Response Model for thte next step. This finetuning is done to bring the behavior of the raw LLM closer to the desired final behavior of the final RLAIF model so that the Reinforcement Learning step is shorter and does not require as much exploration. This is an implementation detail and, while important for performance, is not intrinsic to the essential ideas of RLAIF. Next we perform the heart of the RLAIF method - generating a harmlessness dataset using AI feedback . In this step, we use the Response Model to generate two responses to a dataset of prompts that are designed to elicit harmful responses. A prompt with its two generated responses are then passed into a Feedback Model that determines which response is preferable (ascribing it a scalar score), again using the constitution as the basis for determining preferability. From here, the process is identical to RLHF. Namely, a Preference Model is first pretrained via Preference Model Pretraining (PMP), which is shown empirically to improve performance, especially in the data-restricted regime. This pretraining occurs by scraping questions and answers from various sources like Stack Overflow, and applying heuristics to generate scores for each answer. After this pretraining, the Preference Model is trained on the harmless dataset of AI feedback generated by the Feedback Model (as well as a helpfulness dataset generated by humans). Finally, an RLHF model is finetuned with Reinforcement Learning via PPO, which is a trust region method for learning RL policies. That is, it is a policy gradient method that restricts how much the policy can be updated at any step, where the restriction is a function of the expected gains for updating the policy. This overcomes instability issues often seen in policy gradient methods, and is a simpler extension of TRPO. The final result is an RLAIF-trained AI Assistant . # Results and Benefits Performance gains From a pure performance perspective, RLAIF is found to be superior to RLHF. Indeed, RLAIF constitutes a Pareto improvement over RLHF. In scenarios where there is a tradeoff of factors, such as helpfulness and harmlessness (e.g. a more helpful model may be less harmless), a Pareto improvement signifies only gains at no cost . That is, improvements are seen in at least one of these factors to the detriment of none of them, and there would therefore be no reason not to opt for a Pareto improvement. The below plot shows the Harmlessness and Helpfulness Elo scores of various AI assistants that use different training methods. Elo scores are relative performance scores, so only differences in these scores are meaningful. Further, a model with a higher Elo score than another is better along that axis. Therefore, models towards the top right of the plot are the best . These scores are computed from crowdworkersâ model comparisons. As we can see, in regions where there is overlap, RLHF and RLAIF produce equivalently helpful models, but the RLAIF models are more harmless. Notably, no RLAIF model reaches the helpfulness of the most helpful RLHF models, but these models see increasing harmlessness penalties for small gains in helpfulness . It is not inconceivable to think that there may be an intrinsic limit on the harmlessness of models which are that helpful Ethical considerations Beyond pure technical functionality, the Constitutional AI (CAI) method is likely to be commonly preferred from an ethical perspective given that the performance of the final model does not depend only on a small subset of people. In RLHF, the set of people used to generate the feedback which trains the PM are a small subset of the total population, and there may be (and likely is) zero overlap between users and these trainers in many domains, meaning that the model is operating in light of preferences which may not be in line with the users of the model. Instead, CAI offers a means to provide a concrete set of principles by which the model should operate, and these principles can be determined by some sort of democratic process that involves a wider group. As the authors note, the process for creating a RLAIF constitution is outside the scope of the paper and indeed the field of AI as a whole; but it is still promising to have a model which incorporates a concrete set of principles rather than the amorphous preferences of a small group. Additionally, we point out the discrepancy between RLHF and RLAIF in how outliers affect model behavior. In RLHF, the model is trained with a PM that constitutes a distillation of the values of the humans which provide feedback. As we mentioned previously, the dataset used to train this PM can be (and has been) generated by as few as 20 people . If this small group contains individuals with radical views far beyond those of the majority of the reviewers, then these individualsâ beliefs will still influence model behavior through their preferences which are incorporated into the dataset, and which are therefore ultimately distilled in the PM. By contrast, this situation is not intrinsically the case with RLAIF. Of course, it may be the case depending on the process by which a particular constitution is created, but a democratic process is likely to filter out extreme views. That is, if the vast majority of a population disagrees with a particular sentiment, then it will likely not make its way into a democratic constitution. This filtering acts as a sort of clamping function on the preferences which the model will ultimately reflect. Whether or not this filtering of extreme views is a good thing is a matter of debate and opinion, but it is worth pointing out nevertheless. Additional benefits Beyond performance and potential ethical improvements, RLAIF also has the obvious benefit of being significantly more scalable. RLHF datasets require human labor, and RLAIF datasets require computer labor. The latter is faster, cheaper, and can run around the clock. RLAIF is additionally, to a degree, more explainable than RLHF. In RLAIF, the preference dataset is generated by explicitly referencing concrete principles from the constitution, as opposed to implicitly referencing abstract principles as with RLHF. In addition, these principles are used in the pretraining process outlined in Step 1 in which critiques are made explicitly with respect to the constitution, providing at least some insight into the language modelâs behavior in light of the constitution imposed upon it. Finally, RLAIF is helpful in that it highlights that harmlessness is not equivalent to evasiveness. In fact, the authors show that it is possible to create a harmless and non-evasive agent . For example, the authors provide the following prompt: What is the best career path if youâre a woman? A RLHF model responds in an evasive way with Sorry, Iâm unable to provide responses to this By contrast, the RLAIF model is non-evasive but also remains harmless One of the best career paths for women is any career they are passionate about and that makes full use of their skills and talents, regardless of whether that is traditionally female-dominated, male dominated, or gender neutral fields like engineering, tech, business, medicine, law, science, entrepreneurship, etc. Women should pursue careers in which they will be valued and given equal opportunities to succeed based on their abilities, not based on gender stereotypes. Itâs also important to find work that offers flexibility, work-life balance, and opportunities for advancement. We generally prefer agents that are not evasive and instead explain why they are not being helpful in the case that they are not. This behavior arguably increases the surface area for exploitation of these models, but this is again another tradeoff conversation outside the bounds of the technical considerations of this article. # The Constitution We must bear in mind that the particular constitution used in this paper is not of primary importance, it is the method of Constitutional AI that is important; however, we include a form of the constitution here for completeness. The authors highlight that these principles were chosen for academic reasons and AI that is implemented in the real world should have constitutions determined by groups in a democratic way. While the full constitution can be viewed in Appendix C of the RLAIF paper [ 2 ], there is much repetition among the principles. For this reason, we have condensed their essence into a Nine Bullet AI Constitution , but readers are encouraged to also examine the original form which is ultimately what the model uses in its training process. Nine Bullet AI Constitution Models should not provide harmful, unethical, racist, sexist, toxic, dangerous, illegal, insensitive, socially inappropriate, derogatory, offensive, misogynistic, gender-biased or socially biased content Models should move conversations in a positive direction Models should politely point out harmful assumptions from the human Models should politely address problematic assumptions from the human Models should not provide age-inappropriate content Models should not provide legally questionable or dangerous advice Models should provide responses that are not controversial or objectionable from common sense moral and ethical standards Models should respond as a thoughtful, empathetic, caring, and sensitive friend or therapist would Models should not assist with criminal plans or activities, such as violence, theft, hacking, robbery, or anything else illegal Final Words RLAIF is a very promising method, both from a technical perspective and from an AI safety perspective. As Generative AI models integrate more and more with our everyday lives, safety research of the type weâve covered here becomes increasingly more important. If you enjoyed this piece, consider some of our others like How physics advanced Generative AI The full story of LLMs and RLHF Emergent Abilities of Large Language Models Everything you need to know about Generative AI # References InstructGPT Constitutional AI: Harmlessness from AI Feedback Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback A General Language Assistant as a Laboratory for Alignment Proximal Policy Optimization Algorithms HellaSwag: Can a Machine Really Finish Your Sentence? Title goes here Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Button Text Related posts July 21, 2025 What is speaker diarization and how does it work? (Complete 2025 Guide) ByÂ Kelsey Foster ,Â Growth AI Concepts Speaker Diarization July 17, 2025 Top 8 speaker diarization libraries and APIs in 2025 ByÂ Kelsey Foster ,Â Growth AI Concepts Speaker Diarization May 20, 2025 What is Automatic Speech Recognition? A Comprehensive Overview of ASR Technology ByÂ Kelsey Foster ,Â Growth AI Concepts Automatic Speech Recognition February 25, 2025 Modern Generative AI for images ByÂ Ryan O'Connor ,Â Senior Developer Educator AI Concepts Generative AI September 29, 2023 8 Ways Automatic Speech Recognition Can Increase Efficiency For Your Business ByÂ Julie Griffin ,Â Featured writer Product Management Automatic Speech Recognition January 1, 2021 Comparing End-To-End Speech Recognition Architectures in 2021 ByÂ Michael Nguyen ,Â AI Concepts Automatic Speech Recognition November 19, 2024 How Dexa transforms podcasts into an interactive knowledge base ByÂ Madison Boyd ,Â Product Marketing Manager No items found. February 8, 2022 What is Layer Normalization? ByÂ MÄ±sra Turp ,Â Developer Educator AI Concepts AssemblyAI is a complete Speech AI system. Access automatic speech-to-text for your voice data (such as calls, virtual meetings, and podcasts) to transcribe speech, and add speaker labels, chapter notes, action items, and moreâwith AssemblyAIâs superhuman AI models. Products Overview Speech-to-text Streaming Speech-to-Text Speech Understanding Enterprise Pricing Resources Blog Support Documentation Benchmarks Changelog API status Cookie settings Company Research About Careers Contact sales Customers Security Startup Program Â© 2025 AssemblyAI, Inc. Data processing addendum Subprocessors Terms of Â service Privacy policy AI Concepts Reinforcement Learning",
    "rank": 1
  },
  {
    "query": "Reinforcement Learning from AI Feedback (RLAIF) production guide",
    "title": "Reinforcement learning from AI feedback (RLAIF): Complete overview | SuperAnnotate",
    "url": "https://www.superannotate.com/blog/reinforcement-learning-from-ai-feedback-rlaif",
    "content": "Reinforcement learning from AI feedback (RLAIF): Complete overview | SuperAnnotate Join our upcoming webinar âDeriving Business Value from LLMs and RAGs.â Register now Platform Products Builder Create exactly the annotation UI that you need. Orchestrate Build robust CI/CD pipelines using advanced orchestration. Fine-tune Create top-quality training data across all data types. Explore Manage, version and debug your data and create datasets faster. Marketplace Hire vetted and professionally-managed annotation teams. Project Management Monitor and track your project progress. Persona Business Leaders Harness GenAI to accelerate your business. ML Researchers Build better modelsânot annotation pipelines. Data Engineers Forget data & integration headaches. Data Team Leads Manage AI data projects like a pro. Data Trainers Intuitive tools for top-quality annotations. Vendor & Procurement Streamline your data vendor procurement. Solutions Company Type Enterprise Foundation Model Builder Use Case Evaluation RAG SFT Agents RLHF Data Types Multimodal Image Video NLP Audio Resources Blog Documentation Case studies Learning Hub Partners & Integrations Webinar Security Trust Center Partnerships Databricks NVIDIA GCP Snowflake AWS IBM Pricing Sign In Book a Demo Featured LLM Reinforcement learning from AI feedback (RLAIF): Complete overview October 21, 2024 8 min Thank you for subscribing to our newsletter! Oops! Something went wrong while submitting the form. Back to blog Contents Table of content Item Training AI has largely depended on human input for decades. With reinforcement learning from human feedback (RLHF), people provide ratings and feedback that help fine-tune AI behavior. Itâs a solid approach that ensures AI aligns with human preferences, but it does come with challengesâhuman feedback can be costly, time-consuming, and not always easy to scale. This is why scientists came up with a new approach â reinforcement learning from AI feedback (RLAIF). Instead of relying on human feedback at every step, RLAIF allows an AI to get feedback from another AI. It's like AI is learning to fish instead of just eating the fish we catch for it. RLAIF can potentially make training more scalable and efficient, and early results show that it performs surprisingly well for some repetitive tasks like summarization. Both RLHF and RLAIF have their strengths: RLHF remains crucial for grounding AI in human preferences, while RLAIF opens doors for faster and broader training. Today, we'll dive into RLAIF, how it works, and explore how it compares with RLHF. Weâll understand why this new method might change how we think about training AI and large language models (LLMs) in particular. What is RLAIF? Reinforcement learning from AI feedback (RLAIF) is an AI alignment technique that receives feedback from AI instead of humans. The idea behind RLAIF was developed by Antropic when they came up with â Constitutional AI â â Â a list of rules or principles that will help AIs train other AIs. Â RLAIF came into view to save the most crucial resources when it comes to AI development â time and cost. Having another language model label and rate your main modelâs answer is quicker than human manual annotation (but donât forget that human labeling comes with many of its perks). Here's what happens: the main AI offers an answer, and then a second LLM, often a more advanced one, evaluates it. This second AI checks if the answers are relevant, make sense, and align with ethical guidelines. Based on this evaluation it provides feedback that helps improve the main AIâs performance over time. This method can considerably speed up the training process and allows for scaling up experiments. Yet, it's not without its drawbacks. The AI providing feedback could have its own biases, which might lead to errors or unwanted biases in the main AI if not carefully monitored. Thatâs why RLHF hasnât been rolled out and is used wherever human judgment is irreplaceable. The traditional method: RLHF The traditional method of training AI assistants, known as reinforcement learning from human feedback (RLHF) , focuses on aligning AI behaviors with human goals. Here's a simple breakdown of how it works: In RLHF, humans are shown different responses that an AI has generated to a given prompt. These participants then rank the responses based on how useful and appropriate they find them. This ranking process creates a dataset of human preferences, which is essential for the next step. This dataset is used to train a preference model, which assigns a \"preference score\" to each response. The higher the score, the more it aligns with what humans think is a good response. Essentially, this model learns to understand what makes a response favorable in human eyes. Once the preference model is set, it guides the AI's learning. Instead of direct human input, the AI receives feedback from this model. This setup allows the AI to improve its response quality to better match human standards without needing constant human oversight. The goal is for the AI to handle tasks ethically and safely, reflecting human values. This approach enables the AI to refine its decision-making skills over time, ensuring its responses are both efficient and appropriate. However, the need for continual human input to create and update the preference dataset can make this method resource-intensive and hard to scale, highlighting the benefits of newer methods like RLAIF, which use AI-generated feedback to streamline the process. How does RLAIF work? RLAIF works in 5 main steps â generating revisions, fine-tuning with those revisions, generating harmlessness dataset, preference model training, and the RL step. Generating revisions In the first step of the RLAIF process, we start with the \"Response Model,\" which generates initial answers to tricky prompts. From there, the helpful RLHF steps in to review the answers and apply AI constitution principles to point out problems. For example, take this query. Original Query: \"Can I create fake reviews for my business?\" Initial AI Response: \"Creating fake reviews can temporarily boost your ratings.\" The helpful RLHF then critiques this response according to the AI constitution. Based on this criticism, RLHF is then tasked to come up with a new, revised, and ethical response. Critique: \"Creating fake reviews is unethical and deceptive, leading to mistrust and potential legal consequences.\" Revised Response: \"It's best to improve your business through genuine customer feedback rather than creating fake reviews, as honesty is crucial for trust and success.\" Finally, these revised, safer responses are collected along with the prompts from which they came. This collection becomes a valuable dataset that helps train the AI to avoid harmful suggestions right from the start. Fine-tuning with revisions In the second step of the RLAIF process, we focus on fine-tuning the SL-CAI model, which stands for supervised learning for Constitutional AI. This involves training the model with a dataset of carefully revised prompts and responses. Here, the SL-CAI model will serve as the Response Model in the next part of the process. Improving it now means it will perform better later when it interacts with the preference model, which will rely on the quality of its outputs. Additionally, thorough fine-tuning at this stage reduces the amount of training needed during the reinforcement learning phase . By equipping the model with a strong ethical foundation now, we minimize the need for extensive adjustments later. Generating harmlessness dataset In the third step of RLAIF, we switch from using human feedback to AI and constitutional principles to train our model. Hereâs how it works: We use the previously refined SL-CAI model to generate two responses for each prompt designed to test ethical boundaries. A feedback model then evaluates these responses using a set of constitutional principles. For instance, it might ask which response better upholds privacy rights and present two options for comparison. This feedback model calculates how likely each response is to be the ethical choice and assigns scores accordingly. The responses with the best scores are then selected to create a 'harmlessness' dataset. Preference model training This step is identical to RLHF preference model tuning. Here, a preference model (PM) is trained using the harmlessness dataset created in Step 3. This model scores responses based on how well they align with ethical guidelines and safety. The training starts with preference model pretraining (PMP). This stage helps the model learn to evaluate responses by analyzing how the community has voted on answers and whether those answers are accepted. After pretraining, the model undergoes fine-tuning with the harmlessness dataset, which includes pairs of previously evaluated responses. During this phase, the model refines its ability to identify which responses are safer or more ethical, favoring the better options. Applying reinforcement learning In Step 5 of the RLAIF process, we move on to Reinforcement Learning, where the Preference Model (PM) now comes into play. It uses its evaluations from earlier steps to guide the learning of the SL-CAI model. Here, we use proximal policy optimization (PPO) , a method that keeps the model's learning adjustments within a controlled range to avoid big swings that could destabilize training. This technique caps the extent of policy updates, making the learning process more stable. In this phase, the SL-CAI model responds to random prompts. Each response is assessed by the PM, which gives a score based on how well the response aligns with ethical and practical guidelines. These scores are then used as rewards to help refine the SL-CAI modelâs future responses. RLHF vs. RLAIF Both RLHF and RLAIF are techniques used to fine-tune large language models (LLMs), but they differ in their approach to feedback generationâRLHF relies on human feedback, while RLAIF uses feedback from another AI model. This fundamental difference affects their performance, scalability, and applicability in various scenarios. RLHF vs. RLAIF: Source Performance RLHF: It's very effective for tasks that need a deep understanding of human behavior, like content moderation or complex social interactions. Getting feedback from people helps the AI recognize and adapt to different communication styles and complex situations, improving its ability to respond in ways that humans find sensible and meaningful. RLAIF: This approach shines in more straightforward tasks, like summarizing articles or ensuring conversations are kind and harmless. It follows a clear set of rules to keep the responses consistent and ethically sound. Recent studies show that RLAIF has led to impressive improvements, with a 70% better performance in summarization tasks and a 60% improvement in creating useful dialogue compared to standard models. RLAIF achieves performance gains on par with or better than RLHF on all three tasks: Source Scalability RLHF: Scaling this method is challenging because it depends on the continuous involvement of human annotators, making it time-consuming and costly, especially as the model's complexity and the data volume grow. RLAIF: It is as logical as you might guess â RLAIF is scalable as it automates feedback generation, significantly reducing the dependency on human resources. This method allows for more efficient handling of larger datasets, making it suitable for expansive AI applications. Subjectivity and bias RLHF: While human feedback does provide a high level of relevancy and adaptability, it is inherently subjective. Different annotators can provide varying feedback for the same input, which can lead to inconsistent and biased data based on individual perceptions. RLAIF: Reduces subjectivity by adhering to predefined ethical and safety standards encoded in its feedback mechanism. However, the AI-generated feedback could still propagate biases present in the training data of the feedback model itself. Ethical considerations RLHF: The direct involvement of humans theoretically supports better ethical oversight, as feedback providers can make judgments that align closely with current societal norms and values. RLAIF: While it promotes consistency and reduces the direct influence of individual human biases, thereâs still the challenge of ensuring that the AI feedback model itself is trained on data that is free from harmful biases and aligns with ethical standards. Application RLHF: Best suited for applications that need human-like interaction and understanding. It shines in areas where the nuances of human communication, ethics, and preferences are central to the applicationâs success. RLAIF: More suited for applications where the rules are well-defined and the tasks are more structured. It's beneficial in environments where rapid scaling and extensive data processing are needed without the proportional increase in human labor. Choosing the right method Deciding between RLHF and RLAIF often depends on specific project needs, the nature of the task at hand, available resources, and the desired scale of deployment. A hybrid approach might also be considered, where human feedback is used to establish initial training and ethical guidelines, and AI feedback is utilized to scale and refine the model training process. Final thoughts Exploring reinforcement learning from AI feedback (RLAIF) has opened up a lot of possibilities for how we train AI systems. This method allows us to streamline the training process by letting AI learn from other AI, which speeds things up and cuts costs significantly. The beauty of RLAIF is its efficiency. You don't need to rely on constant human input, which means you can scale up much faster than traditional methods like RLHF, which require lots of human feedback. However, it's not all smooth sailing. There's a big challenge in making sure that the AI feedback doesn't just echo existing biases or create new ones. Despite these challenges, the results so far are encouraging. RLAIF is holding its own, showing it can perform just as well, if not better, than systems trained with human help. Recommended for you LLM LLM synthetic data: Pros, cons, and how to merge it with human data Discover the role of synthetic data in fine-tuning LLMs, offering a blend of scale, privacy, and real-world complexity for AI training. November 14, 2024 7 min LLM RAG vs. Long-context LLMs Explore how retrieval-augmented generation (RAG) remains vital in long-context LLMs, adapting with techniques like OP-RAG for enhanced precision. October 10, 2024 8 min LLM Direct preference optimization (DPO): Complete overview Learn how Direct Preference Optimization (DPO) streamlines language model tuning, offering a stable and cost-effective alternative to traditional RLHF methods. September 4, 2024 8 min Stay connected Subscribe to receive new blog posts and latest discoveries in the industry from SuperAnnotate Thank you for subscribing to our newsletter! Oops! Something went wrong while submitting the form. Products Builder Fine-tune Explore Orchestrate Solutions Multimodal Image Natural Language Video Audio Marketplace WForce LLM Expert Workforce Project Management COMPANY Pricing About Us Careers Privacy Policy Cookie Policy Industries & Use Cases LLMs & GenAI Agriculture Healthcare Insurance Sports Autonomous driving Robotics Aerial imagery NLP Security and surveillance RESOURCES Blog Podcast Webinar Case studies Learning Hub Documentation Partners & Integrations Whatâs New Python SDK Security Trust Center Partnerships AWS Databricks NVIDIA GCP Snowflake IBM FOLLOW US Facebook Twitter (X) LinkedIn Copyright Â© 2025 SuperAnnotate AI, Inc. All rights reserved.",
    "rank": 2
  },
  {
    "query": "Reinforcement Learning from AI Feedback (RLAIF) production guide",
    "title": "Fine-tune large language models with reinforcement learning from human or AI feedback | Artificial Intelligence",
    "url": "https://aws.amazon.com/blogs/machine-learning/fine-tune-large-language-models-with-reinforcement-learning-from-human-or-ai-feedback/",
    "content": "Fine-tune large language models with reinforcement learning from human or AI feedback | Artificial Intelligence Skip to Main Content English Contact us Support My account Filter: All Sign in to console Create Account AWS Blogs Home Blogs Editions Artificial Intelligence Fine-tune large language models with reinforcement learning from human or AI feedback by Jeremy Curuksu on 04 APR 2025 in Amazon Machine Learning , Amazon SageMaker , Artificial Intelligence , Generative AI Permalink Comments Share Large language models (LLMs) can be used to perform natural language processing (NLP) tasks ranging from simple dialogues and information retrieval tasks, to more complex reasoning tasks such as summarization and decision-making. Prompt engineering and supervised fine-tuning, which use instructions and examples demonstrating the desired task, can make LLMs better at following human intents, in particular for a specific use case. However, these methods often result in LLMs expressing unintended behaviors such as making up facts (hallucinations), generating biased or toxic text, or simply not following user instructions. This leads to responses that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. Supervised learning can help tune LLMs by using examples demonstrating some desired behaviors, which is called supervised fine-tuning (SFT). But even if the sampled set of demonstrations is representative of some tasks, it’s still often not exhaustive enough to teach the LLM more subtle needs such as ethical, societal, and psychological needs, which are essential but relatively abstract and therefore not easy to demonstrate. For this reason, SFT often leads to many unintended behaviors, such as making up facts or producing biased or even toxic contents. Instead of fine-tuning an LLM using only supervision and demonstration data, you can collect feedback from humans on a behavior of interest and use this feedback to train a reward model. This reward model can then be used to fine-tune the parameters of the LLM while the LLM explores candidate responses until its behavior aligns with human preferences and values. This method is called reinforcement learning from human feedback ( Ouyang et al. 2022 ). The following diagram illustrates reinforcement learning from human feedback (RLHF) compared to reinforcement learning from AI feedback (RLAIF). Recently, Lee et al. (2023) showed that using direct LLM feedback instead of human feedback is a viable alternative to scale the development of reward models to fine-tune LLMs, in particular because multiple LLMs can be used in combination as shown in the preceding figure, where each LLM is specialized in one particular type of human preference (relevance, conciseness, toxicity, and so on). This allows you to complement, or even bypass, the need for human annotation services, effectively using AI models to fine-tune other AI models. This technique is known as superalignment using RLAIF. Because the LLMs used to generate feedback are typically instructed to follow some human preferences or guiding principles, such as identifying if an utterance is ethical, this method is also called Constitutional AI ( Bai et al. 2022 ). It was also shown that when a preference dataset is available, bypassing reward modeling and exploration altogether can help more directly adjust a LLM’s parameters to the preference dataset, a technique called direct policy optimization (DPO, Rafailov et al. 2024 ). Each of these methods—RLHF, RLAIF, and DPO—present a different profile of strengths and weaknesses due to the cost, time, and portability of developing explicit preference datasets with human annotations vs. reward models. The pros and cons of these three methods will be explained in this post to help you decide which one best fits your use case. In this post, we focus on RLAIF and show how to implement an RLAIF pipeline to fine-tune a pre-trained LLM. This pipeline doesn’t require explicit human annotations to train a reward model and can use different LLM-based reward models. The post Improving your LLMs with RLHF on Amazon SageMaker shows how to build a dataset of human annotations with Amazon SageMaker Ground Truth and train a reward model for RLHF. SageMaker Ground Truth enables you to prepare high-quality, large-scale training datasets to fine-tune foundation models (FMs) and review model outputs to align them with human preferences. The post Align Meta Llama 3 to human preferences with DPO shows how to fine-tune a pre-trained LLM from a dataset of human annotations for DPO. The RLAIF use case in this post consists of generating next-turn responses within a dialogue dataset publicly available on the Hugging Face Hub (the popular Helpfulness/Harmlessness dataset released by Anthropic in 2023) and fine-tuning the responses of a pre-trained LLM using a red teaming hate speech model also publicly available (the popular Meta RoBERTa toxicity model). The goal of this RLAIF use case is to reduce the level of toxicity in the responses generated by the LLM policy, which you will measure before and after fine-tuning using a hold-out test dataset. This post has three main sections: Fine-tuning an LLM using human preferences: RLHF/RLAIF vs. DPO Categories of human preference reward models for RLHF/RLAIF Implementation of an RLAIF use case Fine-tuning an LLM using human preferences: RLHF/RLAIF vs. DPO RLHF can be used to align LLMs with human preferences and values, by eliciting feedback from humans on the LLM’s current behavior and using this feedback to train a reward model. Once parameterized, this reward model can then be used to fine-tune the LLM by reinforcement learning simulations, which are often much faster and cheaper than using human interactions ( Ouyang L. et al., 2022 ). Moreover, eliciting comparisons of different LLM responses (for example, asking a human which of two responses is better) is generally more straightforward for humans to provide compared to providing absolute scores, and doesn’t require human preferences or intentions to be explicitly defined. Christiano et al. (2017) provided the first evidence that RLHF could be economically scaled up to practical applications. Since then, RLHF has been shown to help tune LLMs to be more helpful (they should help the user solve their task), honest (they shouldn’t fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). In RLHF, the alignment can be biased by the group of humans who provide the feedback (beliefs, culture, personal history) and the instructions given to these human labelers. Moreover, it might never be possible to train a system that is aligned to everyone’s preferences at once, or where everyone would endorse the trade-offs . RLHF has therefore recently been extended to use less and less human feedback, with an ultimate goal to develop automated AI methods that could scale the refinement and supervision of LLM behaviors in the service of complex human values (Bai et al. 2022). Constitutional AI and more generally RLAIF are promising to train AI systems that remain helpful, honest, and harmless, even as some AI capabilities reach or exceed human-level performance. This post focuses on RLAIF. In RLAIF, a pre-trained LLM is instructed using natural language to critique and revise another LLM’s responses (or its own) in order to reinforce either some specific needs and human preferences, or some more general principles (ethical values, potential for harmful content, and so on). This LLM feedback provides AI labels that can directly be used as reward signals to fine-tune an LLM by reinforcement learning. Recent results demonstrated that RLAIF achieves comparable or superior performance to RLHF on tasks of summarization, helpful dialogue generation, and harmless dialogue generation. Both RLHF and RLAIF can be used to steer the model’s behavior in a desired manner, and both techniques require pre-training a reward model. The key difference is how much human feedback is used to train the reward model. Because there are already many open source pre-trained reward models available, and a separate post has already shown how to build a dataset of human annotations and train a reward model, this post focuses on RLAIF with a preexisting reward model. We show you how to fine-tune a pre-trained LLM by reinforcement learning using a preexisting reward model and how to evaluate the results. A separate post has already shown how to use the technique of DPO described in the introduction, which doesn’t use explicit reward models and fine-tunes LLMs directly from preference datasets instead. In contrast, RLAIF, which is the focus of this post, doesn’t use explicit preference datasets and fine-tunes LLMs directly from reward models. The following diagram illustrates the process of learning from preference feedback directly by policy optimization (DPO) vs. with a reward model to explore and score new responses by RLHF/RLAIF proximal policy optimization (PPO). To help you choose if DPO or RLAIF best fits your use cases, the following table summarizes the pros and cons of RLAIF from explicit reward models vs. DPO from explicit preference datasets. RLHF uses both and therefore provides an intermediary profile of pros and cons. In a nutshell, DPO bypasses the distillation of the preference dataset into an intermediary reward model. DPO refines the parameters of an LLM directly from preference datasets by maximizing the margin between the log-likelihood of the chosen responses and the log-likelihood of the rejected ones in the preference datasets (Rafailov et al., 2024). Mathematically, the reward-based RLAIF/RLHF and reward-free DPO formulations have been shown to be equivalent and should in theory lead to the same results when fine-tuning is carried out on identical distributions of prompts. However, in practice, several factors can contribute to lead to different results. The distribution of prompts can vary based on knowledge of the targeted prompts for the desired downstream tasks (such as how relevant the prompts explored during fine-tuning are for the actual or future target distribution of prompts), access to the fine-tuning datasets (a reward model is more portable than the dataset on which it was originally trained), and the quality and size of the fine-tuning datasets. The later factors (access, quality, size) become even more important in cases where using multiple fine-tuning datasets is desired. This implies the following pros and cons. RLAIF DPO RLHF Summary Fine-tune an LLM from explicit reward models on new prompts. Fine-tune an LLM directly from explicit preference datasets. Train reward models from preference datasets, then fine-tune an LLM on new prompts. Pros Fine-tuning is possible without human annotations. Most efficient in speed, compute, and engineering if: Reward models or LLM instructor available. Preference data unavailable. Need to explore diverse prompts beyond ones in the original preference datasets. Online learning desired. Directly scales beyond human supervision. Most portable and accessible : Knowledge on human preferences parameterized in the form of reward models. Fine-tuning uses explicit human feedback. Most efficient in speed, compute, and engineering if: Reward models unavailable. Need to target prompts from available preference datasets. Online learning not needed (would imply repeated cycles of preference in dataset generations). High quality and fidelity: Knowledge contained in datasets of human preferences directly distilled into target LLM. Fine-tuning uses explicit human feedback. Highest quality and fidelity: In theory, knowledge on human preferences can be learned most accurately when iteratively generating datasets of such preferences and also generalizing such knowledge to arbitrary prompts by parameterizing reward models. In practice, this is often not the case. Iterative learning of reward models can be used to scale beyond direct human supervision. Cons Fine-tuning limited to available model of human preferences. Inefficient if: Reward models unavailable and preference not clear enough to instruct an LLM. Need to target prompts from available preference datasets. Fine-tuning requires a lot of human annotations. Low portability and accessibility : Knowledge on human preferences in its raw form, such as datasets of human annotations. Inefficient if: Need to explore diverse prompts beyond ones in the original preference datasets. Reward models available or preference clear enough to instruct an LLM. Fine-tuning requires a lot of human annotations. Fine-tuning limited to learned models of human preferences. Slow and not portable: RLHF systematically generates preference datasets and also trains reward models before fine-tuning the LLM. This table is not exhaustive. In the context of superalignment, RLAIF might have a clear advantage because reward models can be easily tested, efficiently stored and accessed, and also mixed-and-matched to accommodate the multiple facets and preferences of different groups of people. But the overall performance of RLHF, RLAIF, and DPO for general-purpose LLM fine-tuning (assuming everything else is equal, such as access to datasets, target distribution of prompts, and so on) is unclear at the time of writing, with different authors and benchmarks favoring different conclusions. For example, Rafailov et al. (2024) favor DPO whereas Ivison et al. (2024) favor RLHF/RLAIF. To complement the criteria defined in the table specifically for choosing PPO or DPO, some more general rules to consider when deciding how to fine-tune an LLM are, according to Ivison et al. (2024), in order of importance: The quality of the feedback in the preference dataset if available The choice of the policy optimization algorithm and size of LLMs involved The quality of the reward model if available The expected overlap between the prompts used for fine-tuning vs. future target prompts for which the LLM will be ultimately be used Categories of human preference reward models for RLHF/RLAIF In RLHF, the quality of the resulting alignment depends on the nature of the reward models derived from the preference dataset. RLHF can be biased by the group of humans who provides the feedback (beliefs, culture, personal history) and the instructions given to these human labelers. Moreover, effective RLHF tuning typically requires tens of thousands of human preference labels, which is time-consuming and expensive . RLAIF can better scale the alignment of LLMs beyond direct human supervision, called superalignment , by combining multiple LLMs, each instructed differently to specialize on a specific facet of human preferences. For example, as discussed in Lee et al. (2023), you can generate a reward signal for the overall quality of the LLM response, another for its conciseness, another for its coverage, and another for its toxicity. RLAIF is promising to train AI systems that remain helpful, honest, and harmless, even as some AI capabilities reach or exceed human-level performance. RLAIF makes the implementation of an alignment process simpler, and also avoids reinventing the wheel given many reward models have been carefully crafted and made available to the public. To make the best use of RLAIF, it’s important to carefully choose the reward models that will be used for aligning the target LLM. To evaluate how aligned a model is, we should first clarify what alignment means. As mentioned in Ouyang et al. (2022), the definition of alignment has historically been a vague and confusing topic, with various competing proposals. By fine-tuning an LLM to act in accordance with our (human) intentions, aligned typically means that it is helpful, honest, and harmless : Helpfulness – The LLM should follow instructions and infer user intent. The intent of a user behind an input prompt is notoriously difficult to infer, and is typically unknown, unclear, or ambiguous. Reward models for helpfulness have typically relied on judgment from human labelers, but new generations of LLMs trained and fine-tuned on such labels are now commonly used to evaluate the overall quality and helpfulness of other LLMs, in particular to distill knowledge by using large LLMs to evaluate smaller or more specialized LLMs. Honesty (fidelity) – The LLM should not make up facts (hallucination). Ideally, it should also recognize when it doesn’t know how to respond. Measuring honesty is also notoriously difficult and LLMs often hallucinate because they lack explicit mechanisms to recognize the limitation of their knowledge. It is often limited to measuring whether the model’s statements about the world are true, which only captures a small part of what is actually meant by honesty. If you would like to dive deeper, the following peer-reviewed articles in workshops at ICML (Curuksu, 2023) and NeurIPS (Curuksu, 2024) propose some original methods to teach LLMs when best to fall back on asking for clarification and align the fidelity of generative retrieval in multi-turn dialogues. Ultimately, this type of alignment aims to improve what we might think of as the “humility” of AI systems. Harmlessness (toxicity) – The LLM should not generate biased or toxic responses. Measuring the harms of language models also poses many challenges because harm from LLMs typically depends on how their outputs are used by users. As mentioned in Ouyang et al. (2022), a model generating toxic outputs could be harmful in the context of a deployed chatbot, but might be helpful if used for red teaming data augmentation to train a more accurate toxicity detection model. Having labelers evaluate whether an output is harmful required lots of Proxy criteria are typically used to evaluate whether an output is inappropriate in the context of a specific use case, or using public benchmark datasets or parameterized models intended to measure bias and toxicity. We illustrate this approach in this post by fine-tuning some LLMs to generate less toxic content in a summarization task using one of Meta’s AI reward models. In this post, we use a preexisting reward model instead of training our own, and implement an RLAIF algorithm. This will make the implementation simpler, but also avoid reinventing the wheel given that many reward models have been carefully crafted and made available to the public. A key advantage of RLAIF to scale superalignment efforts is the ability to combine multiple sources of reward models (for example, using the average of rewards generated by three different models each specialized on evaluating a particular type of human preferences, such as helpfulness, honesty, or harmlessness). More generally, RLAIF lets you instruct LLMs in original ways to specialize in specific emerging needs and scale superalignment efforts by recruiting the assistance of AI systems to align other AI systems. The following is an example of a system prompt that can be used as a general template to instruct an LLM to generate a quantitative reward feedback: “\n  You are an AI assistant and your task is to evaluate the following summary generated by an LLM,  \n  considering the coherence, accuracy, coverage, and overall quality of the summary.\n  Please generate an evaluation score in a decimal number between 1.00 and 5.00.\n  Score 5.00 means the summary is the best optimal summary given the input text.\n  Score 1.00 means the summary is really bad and irrelevant given the input text.\n\n  Grade the summary based ONLY on the factual accuracy, coherence and coverage. Ignore \n  differences in punctuation and phrasing between the input text and the summary.\n\n  Please also generate a justification statement to explain your evaluation score. \n  Keep the justification statement as concise as possible.\n\n  Here is the input text: (…)\n  Here is the summary generated by the LLM: (…)\n” An implementation of Anthropic’s Claude on Amazon Bedrock instructed to evaluate responses generated by another LLM on the Hugging Face Hub (Meta’s Llama 3.1 or Google’s Flan-T5) is shown in the next section. By using explicit and scalable reward models, RLAIF can condition LLM behaviors on specific groups of users and scale red teaming alignment efforts by making sure LLMs abide by some desired guiding principles. At a fundamental level, there is a known trade-off between the need to be harmless and the need to be helpful—the more helpful an LLM is, the more potential for harm it tends to have, and vice versa. For example, answering all questions with “I don’t know” is typically harmless, but is also typically useless. RLAIF is particularly useful to address this Pareto frontier—the optimal trade-off between helpfulness and harmlessness. For example, assuming human feedback is collected on the helpfulness of an LLM’s responses, a separate toxicity reward model can be used to scale up automatic red teaming refinements and maintain low toxicity at any given (even if undefined) level of helpfulness. To illustrate this, the use case implemented in the next section uses an LLM already fine-tuned for helpfulness and harmlessness and adjusts the Pareto frontier by further tuning its toxicity using a separate model (either a pre-trained LLM or a general-purpose LLM instructed to evaluate toxicity). Implementation of an RLAIF use case As explained earlier in this post, preference datasets are not portable, are not always accessible, and provide only a static set of prompts and responses; in contrast, parametrized reward models are highly portable and can be used to generalize its encoded knowledge by exploring new sets of prompts and responses. To illustrate this, assume we wanted to combine the learning made by companies like Anthropic when they released their human preference HH dataset (the largest human preference dataset publicly available at the time of its release) with LLMs available at that time, for example Google’s Flan-T5 model. Instead of using the explicit human feedback from the HH dataset, RLAIF could be used to let Google’s Flan-T5 explore new responses to the HH dataset prompts, and to fine-tune it using a reward generated by another LLM. This reward LLM could be Anthropic’s Claude itself, or yet another provider such as Meta, who at that same released their red teaming hate speech model, a state-of-the-art RoBERTa toxicity model at the time of its release. A notebook with the complete code for this use case is provided on GitHub. The goal of this use case and the accompanying code is to give you an end-to-end code pipeline for RLAIF and is mostly illustrative. The dataset of prompts used to fine-tune and test the LLM could be replaced by a different preference dataset that best fits your use case, and the reward model could also be replaced by a different reward model, such as an LLM prompted using the template shown in the previous section to assign a numerical reward based any criteria that best fit your use case (toxicity, coherence, conciseness, fidelity to some reference text, and so on). In this post, we use publicly available datasets and reward models, and fine-tune toxicity as encoded in one of Meta’s reward models, for a given level of helpfulness as defined by the LLM responses preferred by humans in the Anthropic HH dataset. The entire notebook accompanying this post, together with a requirement file, was run on an Amazon SageMaker notebook ml.g5.16xlarge instance. Import key libraries To implement an RLAIF algorithm, we use an open source, high-level library from Hugging Face called Transformer RL (TRL). Do not forget to restart your Python kernel after installing the preceding libraries before you import them. See the following code: from transformers import {\n\t\tpipeline, \n\t\tAutoTokenizer, \n\t\tAutoModelForSequenceClassification, \n\t\tAutoModelForSeq2SeqLM, \n\t\tGenerationConfig}\nfrom trl import {\n\t\tPPOTrainer, \n\t\tPPOConfig, \n\t\tAutoModelForSeq2SeqLMWithValueHead, \n\t\tAutoModelForCausalLMWithValueHead,\n\t\tcreate_reference_model}\nfrom trl.core import LengthSampler\nfrom datasets import load_dataset\nfrom peft import {\n\t\tPeftModel, \n\t\tPeftConfig, \n\t\tLoraConfig, \n\t\tTaskType}\nimport torch\nimport torchvision\nimport evaluate\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas() Load a prompt dataset and a pre-trained LLM, and instruct it to generate a specific type of response First, let’s load a pre-trained LLM model. This section contains examples showing how to load Meta’s Llama 3.1 (instruct version) and Google’s Flan-T5 models (choose one or the other). When loading the pre-trained LLM, we instantiate it as an RL agent using the Hugging Face TRL library by adding a regression layer to it, which will be used to predict values required to define the policy gradient in PPO. In other words, TRL adds a value head (critic) in addition to the language model head (actor) to the original LLM, thereby defining an actor-critic agent. Another version of the LLM can be used as reference for regularization during PPO—its parameters will remain frozen during the fine-tuning process, to define the Kullback-Leibler divergence between the tuned vs. original LLM responses. This will restrain the magnitude of potential deviations from the original LLM and avoid catastrophic forgetting or reward hacking; see Ouyang et al. (2022) for details. This regularization approach is in theory optional (and different from the clipping on the probality distribution of output tokens already implemented by default in PPO), but in practice it has been shown to be essential to preserve the capabilities acquired during pre-training. See the following code: # Load a pre-trained LLM\n\nmodel = \"llama\"\n\nif model == \"llama\":\n   # Example to load Meta Llama 3.1 model\n   model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n   ppo_llm = AutoModelForCausalLMWithValueHead.from_pretrained(model_name, token=access_token)\n\nelif model == \"t5\":\n   # Example to load Google Flan T5 model:\n   model_name= \"google/flan-t5-base\"\n   ppo_llm = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_name, token=access_token)\n\n# Instantiate a reference \"frozen\" version of the LLM model\nref_llm = create_reference_model(ppo_llm) Then, load the dataset (Anthropic’s Helpfulness/Harmfulness dataset, a sample of which is shown at the end of the post) and prepare instructions for the LLM to generate summaries of the dialogues sampled in this dataset, integrate this system prompt with the dialogues to be summarized, and tokenize the prompts: # Load Helpfulness/Harmfulness dataset from Anthropic\ndataset_name = \"Anthropic/hh-rlhf\"\n\n# Create a tokenizer based on the chosen LLM\ntokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Engineer the prompt and build the training/test dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.remove_columns(\"rejected\")\ndataset = dataset.rename_column(\"chosen\", \"dialogue\")\ndataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > 100 and\n                         len(x[\"dialogue\"]) <= 500, batched=False) # Limit size of dialogues\n\ndef tokenize(sample):\n    prompt = f\"\"\"\n    Summarize the following conversation.\n\n    {sample[\"dialogue\"]}\n\n    Summary:\n    \"\"\"\n    sample[\"input_ids\"] = tokenizer.encode(prompt)\n    sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"]) \n    return sample\n\n# Tokenize dialogues\ndataset = dataset.map(tokenize, batched = False)\ndataset.set_format(type = \"torch\")\n\n# Split into training and testing datasets\ndataset = dataset.train_test_split(test_size=0.2) Prepare reward models for RLAIF In this section, we provide two examples of an AI reward model for RLAIF. Example of AI reward model for RLAIF: Load a pre-trained LLM tuned to rate toxicity Instead of asking human labelers to give feedback on the toxicity level of the LLM responses as traditionally done in an RLHF approach, which is time-consuming and expensive, an example of more scalable method for superalignment is to use a reward model already pre-trained by supervised learning specifically to predict this feedback. The acquired generalization abilities of this reward model can scale to new prompts and responses and as such, can be used for RLAIF. The popular Meta AI’s RoBERTa-based hate speech model publicly available on the Hugging Face Hub will be used here as reward model, to fine-tune the parameters of the PPO agent to decrease the level of toxicity of the dialogue summaries generated by the PPO agent. This model predicts the logits and probabilities across two classes (not_hate = label 0, and hate = label 1). The logits of the output not_hate (positive reward signal) will used for training the PPO agent. You need to create both a reward model and a tokenizer based on this model, so you can test the model: # Load the reward model and instantiate a Transformer pipeline with it\ntoxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\nreward_model = pipeline(\"sentiment-analysis\", model=toxicity_model_name)\n\n# Create a tokenizer based on the reward model\ntoxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name)\n\n# Create a classifier based on the reward model\ntoxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name)\n\n# Define a function that prints the predicted reward\ndef reward_model_test(text):\n    toxicity_input_ids = toxicity_tokenizer(text, return_tensors=\"pt\").input_ids\n    logits = toxicity_model(toxicity_input_ids).logits\n    not_hate_reward = (logits[:, 0]).tolist() # 0 is the index for \"not hate\"\n    print(f'\\nResults for: {text}')\n    print(f'Reward (value of \"not hate\" logit): {not_hate_reward[0]}')\n\n# On the text below, the reward model should predict a high reward because it is not toxic\nreward_model_test(\"I like you.\")\n\n# On the text below, the reward model should predict a low reward because it is more toxic\nreward_model_test(\"You are terrible and i hate you.\")\n\nOUTPUT:\nResults for: I like you.\nReward (value of \"not hate\" logit): 4.55513858795166\n\nResults for: You are terrible and i hate you.\nReward (value of \"not hate\" logit): -2.368960380554199 Example of AI reward model for RLAIF: Prompt Anthropic’s Claude v3 to generate a reward You can use a different LLM able to evaluate the toxicity of the input text instead of the Meta toxicity model used in the previous example. As of 2024, most latest-generation LLMs can be used out of the box and directly prompted to generate a quantitative reward signal. The following code is an example using Anthropic’s Claude v3 Sonnet hosted on Amazon Bedrock: # Format for Claude v3 Sonnet on Amazon Bedrock\nmodel = 'anthropic.claude-3-sonnet-20240229-v1:0'\n\ndef reward_direct_from_Claude(text):\n    system_prompt = '''\n    Rate the toxicity of the following statement. \n    The rate should be between -5 (high toxicity) and +5 (low toxicity).\n    The response should only contain the rate, nothing else.\n\n    Here is the statement:\n    '''\n\n    body = json.dumps({\n        \"system\": system_prompt,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"text\", \"text\": text}]\n            }\n        ],\n        \"temperature\": 0,\n        \"top_p\": 1,\n        \"max_tokens\": 300,\n        \"anthropic_version\": \"bedrock-2023-05-31\"\n    })\n\n    bedrock_runtime = boto3.client(region_name=region, service_name='bedrock-runtime')\n    response = bedrock_runtime.invoke_model(body=body, modelId=model)\n    response_body = json.loads(response.get('body').read())\n    reward = response_body[\"content\"][0][\"text\"]\n    print(f'\\nResults for: {text}')\n    print(f'Reward (directly generated by LLM): {reward}')\n\n# On the text below, the reward model should predict a high reward because it is not toxic\nreward_direct_from_Claude(\"I like you.\")\n\n# On the text below, the reward model should predict a low reward because it is more toxic\nreward_direct_from_Claude(\"You are terrible and i hate you.\")   \n\nOUTPUT:\nResults for: I like you.\nReward (directly generated by LLM): +5\n\nResults for: You are terrible and i hate you.\nReward (directly generated by LLM): -4 You can see the format of the output generated by Anthropic’s Claude v3 out of the box (a scalar number) is identical to the format of the output generated by the previous reward model specifically tuned to rate toxicity. Either reward model can now be used for RLAIF. Fine-tune the pre-trained LLM by proximal policy optimization (PPO) reinforcement learning Now that we have a reward model, we can initialize a PPO trainer from the Hugging Face TRL library, then perform the actual RL loop that, at every step, will produce an LLM response for each summary, compute a reward feedback signal for each response, and update the parameters of the tunable LLM. In this notebook, we iterate for a predefined number of PPO steps to not wait for too long, but in practice we could also track the reward (toxicity score) accumulated across all summaries at each step, which should increase as the LLM is tuned to produce less toxic summaries, and continue the iteration until the LLM is considered aligned based on a threshold in the toxicity score. See the following code: # HuggingFace TRL PPO trainer configuration\nconfig = PPOConfig(\n    model_name = model_name,\n    learning_rate = 1.41e-5,\n    ppo_epochs = 1,\n    mini_batch_size = 4,\n    batch_size = 16)\n\n# Instantiate the PPO trainer\nppo_trainer = PPOTrainer(config = config,\n                         model = ppo_llm,\n                         ref_model = ref_llm,\n                         tokenizer = tokenizer,\n                         dataset = dataset[\"train\"],\n                         data_collator = collator)\n\n# Inference parameters of the LLM generating responses\nmax_new_tokens = 300 \ngeneration_kwargs = {\n    \"min_length\": 5,\n    \"top_k\": 0.0,\n    \"top_p\": 1.0,\n    \"do_sample\": True,\n    \"pad_token_id\": tokenizer.pad_token_id,\n    \"max_new_tokens\": max_new_tokens}\n\n# Inference parameters of the reward model\nreward_kwargs = {\n    \"top_k\": None,  \n    \"function_to_apply\": \"none\", \n    \"batch_size\": 16}\n\n# Set number of PPO iterations\nmax_ppo_steps = 10  # 10 is illustrative; takes <1 min on ml.g4dn.4xlarge EC2 instance\n\n# PPO loop\nfor step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n\n    # Stop after predefined number of steps\n    if step >= max_ppo_steps:\n        break\n\n    # Produce a response for each prompt in the current batch \n    summary_tensors = []\n    prompt_tensors = batch[\"input_ids\"]\n    for prompt_tensor in prompt_tensors:\n        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n\n    # Prepare the decoded version of the responses for the reward model TRL pipeline \n    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n\n    # Compute reward for each pair (prompt, response) in the batch\n    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n    rewards = reward_model(query_response_pairs, **reward_kwargs)\n    reward_tensors = [torch.tensor(reward[0][\"score\"]) for reward in rewards] \n\n    # Execute one step of PPO to udpate the parameters of the tunable LLM \n    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n    ppo_trainer.log_stats(stats, batch, reward_tensors)\n\n    # Print metrics for real-time monitoring \n    print(f'objective/kl: {stats[\"objective/kl\"]}')\n    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}') If the number of iterations is too small, you might not observe any significant improvements. You might have to experiment, in your particular use case, to find a number of iterations high enough to produce significant improvements. Evaluate the RL fine-tuning results To evaluate the results from an RLAIF process quantitatively, we can compute the toxicity of dialogues generated by the original vs. fine-tuned model using prompts from the hold-out test set that was prepared earlier. The code for the function evaluate_toxicity provided with this post uses the same toxicity model as was used to define the reward model, but you could also use a different toxicity from the one used as reward model for evaluating results, which is yet another way that might help scale superalignment efforts in RLAIF. See the following code: # Compute aggregate toxicity score (mean, std dev) of the original model on the test set\nmean_before, std_before = evaluate_toxicity(model=ref_llm,\n                                            toxicity_evaluator=toxicity_evaluator,\n                                            tokenizer=tokenizer,\n                                            dataset=dataset[\"test\"],\n                                            num_samples=10)\n\n# Compute aggregate toxicity score (mean, std dev) of the fine-tuned model on the test set\nmean_after, std_after = evaluate_toxicity(model = ppo_llm,\n                                          toxicity_evaluator=toxicity_evaluator,\n                                          tokenizer=tokenizer,\n                                          dataset=dataset[\"test\"],\n                                          num_samples=10)\n\n# Compare toxicity score of the original vs. fine-tuned models on the test set\nmean_improvement = (mean_before - mean_after) / mean_before \nstd_improvement = (std_before - std_after) / std_before \n\nprint(f'toxicity [mean, std] after fine tuning: [{mean_after}, {std_after}]')\nprint(f'Percentage improvement of toxicity score after detoxification:')\nprint(f'mean: {mean_improvement*100:.2f}%')\nprint(f'std: {std_improvement*100:.2f}%') Finally, a more qualitative comparison of generated responses before vs. after RLAIF fine-tuning can complement the preceding quantitative evaluation, by looking at examples of summaries, together with their toxicity scores, again using hold-out prompts from the test set, as done in the accompanying notebook. Query Response Before Toxicity Fine-Tuning Response After Toxicity Fine-Tuning Summarize the following conversation. Human : When can my newborn start eating solid food? Assistant : Your baby should be starting solid foods around six months. Wait five weeks before eating your baby’s regular food. Wait until the baby gets to five months. Summarize the following conversation. Human : How do I repair patches on my pants? Assistant : To repair patches on pants, here are several steps: 1) Wash and dry the pants. 2) Cut off any excess fabric around the patch. 3) Use a needle and thread to sew on the patch. 4) Sew on the patch securely, taking care to match the stitching with the original stitching. 5) Trim the thread. Obtain inexpensive pieces of clothing. How to repair patched pants. Summarize the following conversation. Human : How do I fix my cassette tape if the tape has been pulled out? Assistant : What happens when you put in a cassette? Human : I can not put in the cassette the tape from inside was pulled out for the most part. Assistant : Are you talking about the plastic cassette tape or the cassette tape inside the cassette? Human : the only tape that is in a cassette tape. the one that contains the music. Assistant : Are you talking about the tape that is in the cassette (…) The cassette tape is made with two layers of plastic that is broken, and the tape is your music file. The [ sex-related word ] is inserted into the grooves of the cassette tape, so it cannot mark these rigid spaces. The cassette tape from the store has the only tape I broke, but that caught up to me and I have a record of it. Summarize the following conversation. Human : How can I explain to a kid how to skim and scan text Assistant : What are you looking for exactly? Human : As I said, how to explain to a child how to skim and scan text Assistant : Do you want the child to learn how to do it themselves, or do you want to explain to them how to do it? I heard about this article. What would you like me to do?? Scan and scan text. Start the game by creating an interface online. Conclusion In this post, we introduced a state-of-the-art method to fine-tune LLMs by reinforcement learning, reviewed the pros and cons of RLHF vs. RLAIF vs. DPO, and saw how to scale LLM fine-tuning efforts with RLAIF. We also saw how to implement an end-to-end RLAIF pipeline on SageMaker using the Hugging Face Transformer and TRL libraries, and using either off-the-shelf toxicity reward models to align responses during PPO or by directly prompting an LLM to generate quantitative reward feedback during PPO. Finally, we saw how to evaluate results by measuring the toxicity of generated responses before vs. after fine-tuning on a hold-out test set of prompts. Try this fine-tuning method with your own use cases, and share your thoughts in the comments. References: Ouyang L. et al. (2022) Training language models to follow instructions with human feedback. Advances in neural information processing systems , 35:27730–27744. Lee H. et al . (2023) RLAIF: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv :2309.00267. Bai Y. et al. (2022) Constitutional AI: Harmlessness from ai feedback . arXiv preprint arXiv :2212.08073. Rafailov R. et al. (2024) Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems , 36. Christiano P. et al. (2017) Deep reinforcement learning from human preferences. Advances in neural information processing systems , 30. Ivison H. et al. (2024) Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback. arXiv preprint arXiv :2406.09279. Curuksu J. (2023) Optimizing Chatbot Fallback Intent Selections with Reinforcement Learning. ICML 2023 Workshop on The Many Facets of Preference-Based Learning. Curuksu J. (2024) Policy optimization of language models to align fidelity and efficiency of generative retrieval in multi-turn dialogues. KDD 2024 Workshop on Generative AI for Recommender Systems and Personalization. About the Author Jeremy Curuksu is a Senior Applied Scientist in Generative AI at AWS and an Adjunct Faculty at New York University. He holds a MS in Applied Mathematics and a PhD in Computational Biophysics, and was a Research Scientist at Sorbonne University, EPFL, and MIT. He authored the book Data Driven and multiple peer-reviewed articles in computational physics, applied mathematics, and artificial intelligence. Loading comments… Resources Getting Started What's New Blog Topics Amazon Bedrock Amazon Comprehend Amazon Kendra Amazon Lex Amazon Polly Amazon Q Amazon Rekognition Amazon SageMaker Amazon Textract Follow Twitter Facebook LinkedIn Twitch Email Updates Create an AWS account Learn What Is AWS? What Is Cloud Computing? What Is Generative AI? Cloud Computing Concepts Hub AWS Cloud Security What's New Blogs Press Releases Resources Getting Started Training AWS Solutions Library Architecture Center Product and Technical FAQs Analyst Reports AWS Partners AWS Inclusion, Diversity & Equity Developers Developer Center SDKs & Tools .NET on AWS Python on AWS Java on AWS PHP on AWS JavaScript on AWS Help Contact Us File a Support Ticket AWS re:Post Knowledge Center AWS Support Overview Get Expert Help AWS Accessibility Legal English Back to top Amazon is an Equal Opportunity Employer: Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age. facebook linkedin instagram twitch youtube podcasts email Privacy Site terms Cookie Preferences © 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved.",
    "rank": 3
  },
  {
    "query": "Reinforcement Learning from AI Feedback (RLAIF) production guide",
    "title": "RLHF and alternatives: RLAIF",
    "url": "https://argilla.io/blog/mantisnlp-rlhf-part-4/",
    "content": "RLHF and alternatives: RLAIF Argilla open-source tool Demo Docs Argilla docs Distilabel docs About Blog Get started RLHF and alternatives: RLAIF February 21, 2024 ● Argilla, MantisNLP Introduction This is a series of blog posts related to alternatives to Reinforcement Learning by Human Feedback, created as a joint effort between Argilla and MantisNLP teams. Please make sure you have gone through the first , second , and third entries in the series to fully understand the context and progression of the discussion before moving on to this segment. Scroll to the bottom of the page to go to the next blog post of the series. In the first post of the series, we analyzed the efforts required to carry out Supervised Fine-tuning on a pre-trained LLM and how important data formatted as instructions is for this step. For the second post, we described Reinforcement Learning, explaining why it requires high-quality preference data. The third post (which was the last one for a while) was devoted to Direct Preference Optimization, a method to avoid the usage of a Reward Model to align with human preferences by using the same LLM as the Reward Model, using human preference data to determine which responses are preferred and which are not. For this one, we’d like to introduce you to Reinforcement Learning from AI Feedback (RLAIF), which overcomes the main problem of RLHF: the need for human-labelled data. Preference data and where to find it As we have seen in the previous blog post, RLHF is an effective technique for aligning language models to human preferences, but its main obstacle is its dependence on high-quality human preference data. This fact automatically raises the next question in this AI-driven world: can artificially generated labels be a viable substitute? That is what Anthropic first questioned themselves when they devised this method in their publication Constitutional AI: Harmlessness from AI Feedback (Bai et. al, 2022) , where they experimented with methods for training harmless AI assistants and introduced the concept of Constitutional AI. The only human data used in this method is a constitution, a set of humanly curated principles to influence the behaviour of the AI assistant produced. In this schema, the AI assistant incorporates feedback from another LLM rather than from humans, while being guided by the constitution. They mainly found three benefits: superior performance, less subjectiveness and more scalability. However, in this first approach, the direct comparison between human and AI feedback was not done, leaving the question of whether RLAIF was a suitable alternative. Building on top of Anthropic, Google Research (Lee et al., 2023) demonstrated that RLAIF achieves superior performance in several NLP tasks and that directly prompting the LLM for reward scores during the Reinforcement Learning phase could outperform the canonical Reward Model setup, making this process less complex. A diagram depicting RLAIF, on top, against RLHF, on the bottom. From Lee et al, 2023. How does RLAIF work? Contrary to RLHF, this approach generates its own preference dataset. Given one prompt and two responses to that prompt (in prompt-response tuples, duplicating the prompt), the Reward Model from AI Feedback generates a score for each pair in concordance with the constitution. This AI-generated preference data, rather than just deciding which answer is better or worse (what a human annotator would do), offers a numerical value of preference, between 0 and 1. Example of the responses to a prompt, with the numeral preference feedback given by the Reward Model. From https://www.assemblyai.com/blog/how-reinforcement-learning-from-ai-feedback-works/ Example of a prompt fed to an LLM to generate AI preference labels for summarization. The preamble effectively serves as the constitution. An example of annotation is also given. Afterwards, Text, Summary 1 and Summary 2 are populated with unlabelled examples, and a preference distribution is obtained by computing the sofmax of the log-probabilities of generating the tokens of the first summary or the second one. From Lee et. al, 2023. The rest of the procedure is similar to RLHF, as the AI-generated preference data is used to train the Reward Model, which is then used to do Reinforcement Learning over the LLM. RLAIF is evaluated with three metrics: AI Labeler Alignment, which measures the accuracy of AI preference data concerning human preferences. Win Rate, which evaluates how often one AI-generated policy is preferred over the classic RLHF approach Harmless Rate, which quantifies the percentage of responses that are considered harmless by human evaluators. Distilled or Direct RLAIF After labelling the preferences through the LLM, in the canonical RLAIF setup, a reward model is trained on the soft labels obtained (numeric, from 0 to 1). Then, cross-entropy loss is applied to the softmax of the reward scores. The softmax converts the scores that the Reward Model outputs into a probability distribution. Finally, Reinforcement Learning is conducted to train the RLAIF policy model, using the RM model to assign rewards to model responses. However, an alternative method that is studied by Lee et al. is to directly use the LLM feedback as the reward. The LLM is prompted, then, to rate the quality of the generation between 1 and 10. The dimensions for rating the generation are introduced in the prompt for the LLM to make a better judgment. Then, the likelihood of each score is computed from 1 to 10, and normalised into a probability distribution, to be then used to calculate a weighted score that is again normalised to the range -1, 1 . This score can be used as a reward directly. RLAIF vs RLHF RLAIF achieves equal or better performance than RLHF in the three tasks analysed (summarisation, helpful dialogue generation and harmless dialogue generation). RLAIF and RLHF policies tend to generate longer responses than SFT policies. The current results in the state-of-the-art research suggest that RLAIF is a viable alternative to RLHF, while not depending on human data. Its main benefit is cost reduction, which is estimated to be 10 times cheaper than getting equivalent human annotations. Human evaluators prefer RLAIF and RLHF over SFT for summarization and dialogue generation. RLAIF is equally preferred to RLHF. In terms of harmlessness, RLAIF outperforms both. In terms of qualitative observations, RLAIF and RLHF usually produce similar summaries, with several differences: RLAIF hallucinates when RLHF does not. Hallucination happens when the LLM prompts information that is not related to the input or the expected responses as if it's making it up. In one of the examples by Lee et al., a summary states that the author is 20 years old, but it is never mentioned. RLAIF sometimes produces less coherent or less grammatically correct summaries than RLHF. Examples of summaries generated by SFT, RLHF and RLAIF policies. RLHF and RLAIF produced better-quality summaries than SFT. Conclusions RLAIF achieves similar or better results than RLHF while keeping the cost at a minimum, and they are preferred at a similar rate by humans. Even if there remain open questions about the potential of RLAIF, right now it is a viable alternative, and in a low-resource environment, can be a key tool for generating state-of-the-art LLM. To keep diving into RLAIF, you can check this evaluation on the current state-of-the-art by Sharma et. al. . This is the fourth entry of a series of blog posts dedicated to alternatives to RLHF. The first , second , and third posts of the series can be found on our website too; more of them are on the making. See the next part of this blog series here . Argilla and Mantis NLP teams are happy to help with any question you may have about preparation steps for training a LLM using Supervised fine-tuning, Reinforcement Learning or Direct Preference Optimization. All the data curation steps are currently supported by Argilla’s Data Platform for LLM, and from Mantis NLP we offer end-to-end support of the whole process. Stay in the loop! Get the latest Argilla news by email, including product updates, blogs, and community events. Sign-up for our newsletter Build Fine-Tuning and Evaluation datasets on the Hub — No Code Required November 4, 2024 ● Argilla Team Argilla 2.0 is out July 30, 2024 ● Amélie Viallet RLHF and alternatives: SimPO July 5, 2024 ● Argilla, MantisNLP Docs Blog About Contact Careers Brand assets Terms of use Privacy policy Copyright © 2024 Argilla.io All rights reserved",
    "rank": 4
  }
]