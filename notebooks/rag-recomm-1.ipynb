{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12622653,"sourceType":"datasetVersion","datasetId":7975228}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install faiss-cpu --no-cache-dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T13:20:36.056756Z","iopub.execute_input":"2025-07-30T13:20:36.057118Z","iopub.status.idle":"2025-07-30T13:20:42.925189Z","shell.execute_reply.started":"2025-07-30T13:20:36.057058Z","shell.execute_reply":"2025-07-30T13:20:42.924055Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m248.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.11.0.post1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ─── Cell 1: Imports & load index ─────────────────────────────────────────────\nimport faiss\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\n\n# 1) Load FAISS index & metadata\nindex = faiss.read_index(\"/kaggle/input/indexes/faiss.index\")\nwith open(\"/kaggle/input/indexes/metadata.pkl\", \"rb\") as f:\n    df = pickle.load(f)\n\n# 2) Init your embedder\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T13:23:17.465202Z","iopub.execute_input":"2025-07-30T13:23:17.465582Z","iopub.status.idle":"2025-07-30T13:24:06.786860Z","shell.execute_reply.started":"2025-07-30T13:23:17.465547Z","shell.execute_reply":"2025-07-30T13:24:06.785911Z"}},"outputs":[{"name":"stderr","text":"2025-07-30 13:23:36.057755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753881816.300391      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753881816.378064      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed53685f821342549ae4407f1e7565b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b324564f65d341088aa15e40343b15e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3104cebda6bd4e4590c44c3575fabcc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"581527f49fca468e95f9728f4cbae7a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2d6a08db7a46bd873e57d50cf3bb2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cfaa5fc74ce481baad0b7577a09fa38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1e5a4efdb64034bef9a16f6f03ef68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff324d84e7c493fbb310e4f3d4bb6ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdc7d319117e447e9dfdf2ba2e59a0ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"420f8262ab3d4f499ce16baa22071416"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e466b759d91b4c5ba50bc0c687e8ae0e"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ─── Cell 2: Pure dense‐retrieval search function ─────────────────────────────\ndef search_titles_urls(query: str, k: int = 5) -> pd.DataFrame:\n    \"\"\"\n    Returns the top‐k (title, url) for the given query using\n    MiniLM embeddings + FAISS inner‐product search.\n    \"\"\"\n    qv = model.encode([query]).astype(\"float32\")\n    D, I = index.search(qv, k)\n\n    # grab & dedupe\n    res = (\n        df.iloc[I[0]][[\"title\",\"url\"]]\n        .drop_duplicates()\n        .reset_index(drop=True)\n    )\n    return res\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T13:25:15.254442Z","iopub.execute_input":"2025-07-30T13:25:15.254830Z","iopub.status.idle":"2025-07-30T13:25:15.260552Z","shell.execute_reply.started":"2025-07-30T13:25:15.254798Z","shell.execute_reply":"2025-07-30T13:25:15.259613Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nfrom IPython.display import HTML, display\n\n# 1) Run your search as before\nquery   = \"Attention is all you need\"\nresults = search_titles_urls(query, k=10)\n\n# 2) Convert the 'url' column into clickable links\nresults_html = results.to_html(\n    escape=False,\n    index=False,\n    formatters={\n        \"url\": lambda x: f'<a href=\"{x}\" target=\"_blank\">{x}</a>'\n    }\n)\n\n# 3) Display as HTML\ndisplay(HTML(results_html))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T13:28:16.508159Z","iopub.execute_input":"2025-07-30T13:28:16.508484Z","iopub.status.idle":"2025-07-30T13:28:16.551415Z","shell.execute_reply.started":"2025-07-30T13:28:16.508460Z","shell.execute_reply":"2025-07-30T13:28:16.550558Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d6bbdab2ea4b7c883e36c328c4957f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>title</th>\n      <th>url</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Day 58: Attention Mechanisms — Foundation of Transformer Models | by Adithya Prasad Pandelu | Medium</td>\n      <td><a href=\"https://medium.com/@bhatadithya54764118/day-58-attention-mechanisms-foundation-of-transformer-models-c3139d3c0e79\" target=\"_blank\">https://medium.com/@bhatadithya54764118/day-58-attention-mechanisms-foundation-of-transformer-models-c3139d3c0e79</a></td>\n    </tr>\n    <tr>\n      <td>What is an attention mechanism? | IBM</td>\n      <td><a href=\"https://www.ibm.com/think/topics/attention-mechanism\" target=\"_blank\">https://www.ibm.com/think/topics/attention-mechanism</a></td>\n    </tr>\n    <tr>\n      <td>Attention in Deep Learning, your starting point (with code)</td>\n      <td><a href=\"https://tungmphung.com/attention-in-deep-learning-your-starting-point-with-code/\" target=\"_blank\">https://tungmphung.com/attention-in-deep-learning-your-starting-point-with-code/</a></td>\n    </tr>\n    <tr>\n      <td>Reddit - The heart of the internet</td>\n      <td><a href=\"https://www.reddit.com/r/MachineLearning/comments/qidpqx/d_how_to_truly_understand_attention_mechanism_in/\" target=\"_blank\">https://www.reddit.com/r/MachineLearning/comments/qidpqx/d_how_to_truly_understand_attention_mechanism_in/</a></td>\n    </tr>\n    <tr>\n      <td>The Mathematics of Attention Mechanisms: Building Self-Attention from First Principles | by Abduldattijo | Generative AI</td>\n      <td><a href=\"https://generativeai.pub/the-mathematics-of-attention-mechanisms-building-self-attention-from-first-principles-b9895e9ee491\" target=\"_blank\">https://generativeai.pub/the-mathematics-of-attention-mechanisms-building-self-attention-from-first-principles-b9895e9ee491</a></td>\n    </tr>\n    <tr>\n      <td>All you need (to know) about attention | AndoLogs</td>\n      <td><a href=\"https://blog.ando.ai/posts/all_you_need_to_know_about_attention/\" target=\"_blank\">https://blog.ando.ai/posts/all_you_need_to_know_about_attention/</a></td>\n    </tr>\n    <tr>\n      <td>5 Attention Mechanism Insights Every AI Developer Should Know</td>\n      <td><a href=\"https://shelf.io/blog/attention-mechanism/\" target=\"_blank\">https://shelf.io/blog/attention-mechanism/</a></td>\n    </tr>\n    <tr>\n      <td>Understanding the attention mechanism in sequence models</td>\n      <td><a href=\"https://www.jeremyjordan.me/attention/\" target=\"_blank\">https://www.jeremyjordan.me/attention/</a></td>\n    </tr>\n    <tr>\n      <td>Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</td>\n      <td><a href=\"https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\" target=\"_blank\">https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</a></td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}