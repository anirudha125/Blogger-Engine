{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd46026-af15-4ef4-8c31-f55e9087b96d",
   "metadata": {},
   "source": [
    "# RAG Prep"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b72f8798-addd-4527-951d-59f4c0187701",
   "metadata": {},
   "source": [
    "Idea is to form NER keywords, BM25 indexes, Splade indexes, FAISS dense indexes & a structured metadata for the RAG Pipelin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b58f4-4c75-4943-9ec5-11ef219a390b",
   "metadata": {},
   "source": [
    "### Loading & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5f3fba-3b84-4f45-a554-91a1243cc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475223bf-0875-4433-92a0-9f1eb027268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"naukri_blogs.json\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723cb203-9779-438f-a275-c6af63fc32c1",
   "metadata": {},
   "source": [
    "### NER formation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f196202-7a8c-4341-8320-90adca7b4ac5",
   "metadata": {},
   "source": [
    "- Spacy > NLTK, so using it and only keeping \"ORG\", \"PERSON\"\n",
    "- (Why?) Might help in boosting BM25 scores\n",
    "- (Issues) big sentences are getting passed, so split -> remove numbers, punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5479f169-b299-4c15-915d-90ba21ace7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9fbd12-7e0c-412e-b40c-808a51e057ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ners(text: str) -> List[Dict]:\n",
    "    doc = nlp(text)\n",
    "    ners = []\n",
    "    seen = set()\n",
    "    for ent in doc.ents: # entities\n",
    "        if ent.label_ in [\"ORG\", \"PERSON\"]:\n",
    "            cleaned = \" \".join(ent.text.split()) # text\n",
    "            cleaned = re.sub(r'[^\\w\\s]', '', cleaned) #punctuation remove\n",
    "            cleaned = re.sub(r'\\d+', '', cleaned) # numbers remove\n",
    "            cleaned = cleaned.strip()\n",
    "            if cleaned:\n",
    "                entity = (cleaned.lower(), ent.label_)\n",
    "                if entity not in seen: # remove duplicates\n",
    "                    ners.append({\n",
    "                        \"word\": cleaned,\n",
    "                        \"label\": ent.label_,\n",
    "                        \"score\": 1.0\n",
    "                    })\n",
    "                    seen.add(entity)\n",
    "            \n",
    "    return ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42304afd-4595-4d8a-bc39-595163cc4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ners(data: List[Dict]):\n",
    "    for i, item in enumerate(data):\n",
    "        body_text = item['body']\n",
    "        title_text = item['title']\n",
    "        ner_body = extract_ners(body_text)\n",
    "        ner_title = extract_ners(title_text)\n",
    "        ner_list = ner_body + ner_title\n",
    "        ner_list.append({'word': item['topic'], 'score': 1.0})\n",
    "        item['ner'] = ner_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b6745d7-98b1-417f-9db5-0761f049d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_ners(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97370806-a04d-4678-9d03-c17543a23709",
   "metadata": {},
   "source": [
    "### BM25 Inverted Index Formation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d390ab43-0f5f-4b0f-861e-df920c50464c",
   "metadata": {},
   "source": [
    "- (Why?) Faster BM25 exact keyword search, else O(m*n) complexity now O(n) approximately + no-over head for online-algorithm\n",
    "- Lowercase -> remove punctuations -> tokenize -> remove stop words -> stemming / lammatization\n",
    "- Then add this to the data as \"BM25 index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfffeab2-d689-4e25-9c20-d776c42e21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_keywords(data: List[Dict])-> List[Dict]:\n",
    "    for i, item in enumerate(data):\n",
    "        # lower -> remove punctuation -> remove numbers -> tokenize -> remove stop words -> lemmatization\n",
    "        body_text = item['body']\n",
    "        body_text = body_text.lower()\n",
    "        body_text = re.sub(r'[^\\w\\s]', '', body_text)\n",
    "        body_text = re.sub(r'\\d+', '', body_text)\n",
    "        \n",
    "        title_text = item['title']\n",
    "        title_text = title_text.lower()\n",
    "        title_text = re.sub(r'[^\\w\\s]', '', title_text)\n",
    "        title_text = re.sub(r'\\d+', '', title_text)\n",
    "\n",
    "        btext = nlp(body_text)\n",
    "        ttext = nlp(title_text)\n",
    "\n",
    "        keywords = []\n",
    "        for text in btext:\n",
    "            if not text.is_stop and text.is_alpha and len(text.text)>1:\n",
    "                keywords.append(text.lemma_)\n",
    "        for text in ttext:\n",
    "            if not text.is_stop and text.is_alpha and len(text.text)>1:\n",
    "                keywords.append(text.lemma_)\n",
    "        item['bm25'] = keywords\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a28fa0-5420-48b9-9f48-eb63bb07044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bm25_keywords(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290fd0b-3ba9-4f00-854b-f318b93c632e",
   "metadata": {},
   "source": [
    "### SPLADE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52c86af6-a862-4ab6-a83a-d321b74eedd6",
   "metadata": {},
   "source": [
    "- document condensed to 30,522 word vector |V| basically (using the MLM head for prob prediction of masked token then summing up using log of ReLU of wij (basic splade))\n",
    "- chunked and taken max, else attention has quadratic time complexity + dilution of main hits (but slight loss of context so overlap is needed to counter it)\n",
    "- can we form better semantically logical splits of sentences? (yes but overkill + longer code)\n",
    "- can use exactSDM, softSDM but would be overkill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed0199d-8224-447f-9679-f0fff582e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eca42df6-453f-4887-b29e-35786c98f9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 21:05:53.453455: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-22 21:05:53.465533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750606553.478426 2047716 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750606553.482653 2047716 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750606553.494566 2047716 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750606553.494589 2047716 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750606553.494591 2047716 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750606553.494593 2047716 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-22 21:05:53.498929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splade_tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-v3-distilbert\")\n",
    "splade_model = AutoModelForMaskedLM.from_pretrained(\"naver/splade-v3-distilbert\")\n",
    "splade_model.eval() # set to eval for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe1d1f1-96e6-4894-99e0-1fd6663c12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splade_vectors(data: List[Dict], chunk_size= 128, overlap = 32) -> List[Dict]:\n",
    "    for idx, item in enumerate(data):\n",
    "        text = item['body']\n",
    "        tokens = splade_tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        chunks_text: List[str] = []\n",
    "        for i in range(0, len(tokens), chunk_size - overlap): # go till min of both\n",
    "            chunk_tokens = tokens[i : i + chunk_size]\n",
    "            chunk_text = splade_tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "            chunks_text.append(chunk_text)\n",
    "\n",
    "        splade_vectors: List[Dict[int, float]] = [] # splade vector of each chunk\n",
    "        for chunk_content in chunks_text:\n",
    "            inputs = splade_tokenizer(\n",
    "                chunk_content,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=splade_tokenizer.model_max_length,\n",
    "                truncation=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = splade_model(**inputs)\n",
    "                sparse_embedding = torch.max( # splade formula\n",
    "                    torch.log(1 + torch.relu(output.logits)) * inputs.attention_mask.unsqueeze(-1),\n",
    "                    dim=1\n",
    "                ).values.squeeze(0)\n",
    "\n",
    "            # removing the zero values as it's mostly sparse\n",
    "            non_zero_indices = sparse_embedding.nonzero(as_tuple=True)[0]\n",
    "            non_zero_values = sparse_embedding[non_zero_indices]\n",
    "\n",
    "            temp = {idx.item(): val.item() for idx, val in zip(non_zero_indices, non_zero_values)}\n",
    "            splade_vectors.append(temp)\n",
    "        item['splade'] = splade_vectors\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b2c2ea7-9410-4277-bca7-71f7e8a947af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2097 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "data = splade_vectors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac9fc1e-c6e5-4b20-a86e-bad180e19ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]['splade'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308c9076-78ca-45de-9f7e-0353f5cd2f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['topic', 'title', 'url', 'body', 'ner', 'bm25', 'splade'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175eaf0d-f5a7-4629-805f-d2c3f523c748",
   "metadata": {},
   "source": [
    "### Dense Indexes (FAISS) & Metadata formation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2336ee11-d068-4d6e-8b72-bdc686704ac6",
   "metadata": {},
   "source": [
    "- Security check, removing the documents which have <= 100 tokens (gibrish garbage content)\n",
    "- Faiss vector DB, using qwen3-0.6B as it has higher context window allowing to form one embedding per document\n",
    "- Better embedding -> Better semantic meaning capture, batch_size kept at 32 to avoid OOM cuda issues\n",
    "- vector db = FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d184b768-dd5e-4253-9a4f-d5ce5fa4680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61e283fc-9715-4dd9-b2b1-5f7bd3e741ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ec57889-2c4e-48d1-b87a-7f3be0931a97",
   "metadata": {},
   "source": [
    "The already formed data serves as metadata, so just a faiss index would suffice\n",
    "If we keep track of position of the chunk (data point) then we can go to metadata and get it's other details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f57da91f-e07a-483f-b826-ec188db2611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12a5d070-6293-4994-b9bb-0bd560d32629",
   "metadata": {},
   "source": [
    "The idea of chunking the main content into many parts is dropped, switched to better embeddings which have larger context window (qwen) as it was creating\n",
    "complexities during RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f4c73e-c5fc-4ece-9b18-e2ae06dab552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_split(text, sz= 512, ov = 64) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks, i = [], 0\n",
    "    while i < len(tokens):\n",
    "        j = min(i + sz, len(tokens))\n",
    "        chunks.append(\" \".join(tokens[i:j]))\n",
    "        if j == len(tokens):\n",
    "            break\n",
    "        i = j - ov\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37d70760-5ba0-43f2-83b9-bf883a52827c",
   "metadata": {},
   "source": [
    "######------Can use splits but then have to think how to combine things in the end as SPLADE, BM25 works on the doc level but FAISS works on chunk level-------#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "604c0b6b-e3a7-43e0-b471-cbdc0df4a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(model, chunks, batch_size=32) -> List[List[float]]:\n",
    "    embeddings = []\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        torch.cuda.empty_cache()\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        emb = model.encode(batch, convert_to_tensor=False)\n",
    "        embeddings.extend(emb)\n",
    "        torch.cuda.empty_cache()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e101b420-2eee-44d4-8a79-2195864e457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, data: List[Dict]) -> List[Dict]:\n",
    "    output = []\n",
    "    for i, item in enumerate(data):\n",
    "        title_text = item['title']\n",
    "        body_text = item['body']\n",
    "        total_text = title_text + body_text\n",
    "        \n",
    "        doc_embedding = encode(model, [total_text])[0]\n",
    "\n",
    "        output.append({\n",
    "            'topic': item['topic'],\n",
    "            'title': item['title'],\n",
    "            'url': item['url'],\n",
    "            'body': item['body'], # whole text\n",
    "            'doc_idx': i,         # unique identifier for the original document\n",
    "            'ner': item['ner'],\n",
    "            'bm25': item['bm25'],\n",
    "            'splade': item['splade'],\n",
    "            'embedding': doc_embedding, # single embedding for the whole document\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ac752c6-5595-4a28-ac65-9ffaa3869476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    emb_list = [item['embedding'] for item in data]\n",
    "    embeddings = np.vstack(emb_list)\n",
    "    meta =    [{'topic': item['topic'],\n",
    "                'title': item['title'],\n",
    "                'doc_idx': item['doc_idx'], # to know which document this belongs to so that during RRF it can be merged easily document wise\n",
    "                'url': item['url'],\n",
    "                'body': item['body'],\n",
    "                'ner' : item['ner'],\n",
    "                'bm25': item['bm25'],\n",
    "                'splade': item['splade'],\n",
    "    } for item in data]\n",
    "    return embeddings, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0249757f-d6e8-4d73-a36e-c1a446847f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path, obj):\n",
    "    with open(path, 'wb') as f:\n",
    "        pkl.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfa60bc8-a717-4c53-9acd-7fbf7f7c7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = process(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94b0de30-a64f-431f-acc8-57af1d8f140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, metadata = extract(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f4dca5-d0af-4e5b-a3a7-9268c2095091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['topic', 'title', 'doc_idx', 'url', 'body', 'ner', 'bm25', 'splade'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8664e3c1-1a0a-4424-836d-b9529372c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "faiss.normalize_L2(embeddings)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc38c605-0ccf-4eed-8cd2-e66434c47502",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('indexes', exist_ok=True)\n",
    "faiss.write_index(index, os.path.join('indexes', 'faiss.index'))\n",
    "save_pickle(os.path.join('indexes', 'embeddings.pkl'), embeddings)\n",
    "save_pickle(os.path.join('indexes', 'metadata.pkl'), metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
